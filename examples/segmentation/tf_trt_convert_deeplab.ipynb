{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"tf_trt_convert_deeplab.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"R6WLV4rAPHi1","colab_type":"code","colab":{}},"source":["import sys\n","import os\n","import urllib\n","import time\n","import shutil\n","import tarfile\n","import tempfile\n","\n","from matplotlib import gridspec\n","from matplotlib import pyplot as plt\n","\n","import numpy as np\n","\n","from PIL import Image\n","\n","from distutils.version import StrictVersion\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0SGnIT5PQBk","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","print(tf.__version__)\n","if StrictVersion(tf.__version__) < StrictVersion('1.14.0'):\n","    raise ImportError('Please upgrade your TensorFlow installation to v1.14.*.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"91mICnrrUovR","colab_type":"code","colab":{}},"source":["from tensorflow.python.compiler.tensorrt import trt_convert"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gBTR5j8FvgfK","colab_type":"text"},"source":["## Define DeepLabModel"]},{"cell_type":"code","metadata":{"id":"u6OzXE2yqm8-","colab_type":"code","colab":{}},"source":["class DeepLabModel(object):\n","    \"\"\"Class to load deeplab model and run inference.\"\"\"\n","\n","    INPUT_TENSOR_NAME = 'ImageTensor:0'\n","    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n","    INPUT_SIZE = 513\n","\n","    def __init__(self, model_path):\n","        \"\"\"Creates and loads deeplab model.\"\"\"\n","        tf.reset_default_graph()\n","        \n","        self.graph = tf.Graph()\n","\n","        graph_def = None\n","        with tf.gfile.GFile(model_path, 'rb') as f:\n","            graph_def = tf.GraphDef.FromString(f.read())\n","\n","        if graph_def is None:\n","            raise RuntimeError('Cannot load inference graph: ' + model_path)\n","\n","        with self.graph.as_default():\n","            tf.import_graph_def(graph_def, name='')\n","\n","        self.sess = tf.Session(graph=self.graph)\n","\n","    def run(self, image):\n","        \"\"\"Runs inference on a single image.\n","\n","        Args:\n","            image: A PIL.Image object, raw input image.\n","\n","        Returns:\n","            resized_image: RGB image resized from original input image.\n","            seg_map: Segmentation map of `resized_image`.\n","        \"\"\"\n","        width, height = image.size\n","        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n","        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n","        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n","        batch_seg_map = self.sess.run(\n","            self.OUTPUT_TENSOR_NAME,\n","            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n","        seg_map = batch_seg_map[0]\n","        return resized_image, seg_map\n","\n","    def run_benchmark(self, image, count=100):\n","        width, height = image.size\n","        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n","        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n","        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n","        \n","        times = []\n","        for i in range(count):\n","            start_tm = time.time()\n","            self.sess.run(\n","                self.OUTPUT_TENSOR_NAME,\n","                feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n","            times.append(time.time() - start_tm)\n","    \n","        print('Inference : {0:.2f} ms'.format(np.array(times).mean() * 1000))\n","\n","    \n","def create_pascal_label_colormap():\n","    \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n","\n","    Returns:\n","        A Colormap for visualizing segmentation results.\n","    \"\"\"\n","    colormap = np.zeros((256, 3), dtype=int)\n","    ind = np.arange(256, dtype=int)\n","\n","    for shift in reversed(range(8)):\n","        for channel in range(3):\n","            colormap[:, channel] |= ((ind >> channel) & 1) << shift\n","        ind >>= 3\n","\n","    return colormap\n","\n","\n","def label_to_color_image(label):\n","    \"\"\"Adds color defined by the dataset colormap to the label.\n","\n","    Args:\n","        label: A 2D array with integer type, storing the segmentation label.\n","\n","    Returns:\n","        result: A 2D array with floating type. The element of the array\n","        is the color indexed by the corresponding element in the input label\n","        to the PASCAL color map.\n","\n","    Raises:\n","        ValueError: If label is not of rank 2 or its value is larger than color\n","        map maximum entry.\n","    \"\"\"\n","    if label.ndim != 2:\n","        raise ValueError('Expect 2-D input label')\n","\n","    colormap = create_pascal_label_colormap()\n","\n","    if np.max(label) >= len(colormap):\n","        raise ValueError('label value too large.')\n","\n","    return colormap[label]\n","\n","\n","def vis_segmentation(image, seg_map):\n","    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n","    plt.figure(figsize=(15, 5))\n","    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n","\n","    plt.subplot(grid_spec[0])\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.title('input image')\n","\n","    plt.subplot(grid_spec[1])\n","    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n","    plt.imshow(seg_image)\n","    plt.axis('off')\n","    plt.title('segmentation map')\n","\n","    plt.subplot(grid_spec[2])\n","    plt.imshow(image)\n","    plt.imshow(seg_image, alpha=0.7)\n","    plt.axis('off')\n","    plt.title('segmentation overlay')\n","\n","    unique_labels = np.unique(seg_map)\n","    ax = plt.subplot(grid_spec[3])\n","    plt.imshow(\n","        FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n","    ax.yaxis.tick_right()\n","    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n","    plt.xticks([], [])\n","    ax.tick_params(width=0.0)\n","    plt.grid('off')\n","    plt.show()\n","\n","LABEL_NAMES = np.asarray([\n","    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n","    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n","])\n","\n","FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n","FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJcz3Ap1PHi4","colab_type":"text"},"source":["# Download the pretrained model"]},{"cell_type":"markdown","metadata":{"id":"Gmfp_La4PHi4","colab_type":"text"},"source":["Download the model configuration file and checkpoint containing pretrained weights by using the following command."]},{"cell_type":"code","metadata":{"id":"OcMahePkPlXt","colab_type":"code","colab":{}},"source":["MODEL_NAME = 'xception_coco_voctrainaug'  # @param ['mobilenetv2_dm05_coco_voc_trainaug', 'mobilenetv2_dm05_coco_voc_trainval', 'mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n","\n","_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n","_MODEL_URLS = {\n","    'mobilenetv2_dm05_coco_voc_trainaug':\n","        'deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz',\n","    'mobilenetv2_dm05_coco_voc_trainval':\n","        'deeplabv3_mnv2_dm05_pascal_trainval_2018_10_01.tar.gz',\n","    'mobilenetv2_coco_voctrainaug':\n","        'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n","    'mobilenetv2_coco_voctrainval':\n","        'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n","    'xception_coco_voctrainaug':\n","        'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n","    'xception_coco_voctrainval':\n","        'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n","}\n","_MODEL_DIRS = {\n","    'mobilenetv2_dm05_coco_voc_trainaug':\n","        'deeplabv3_mnv2_dm05_pascal_trainaug',\n","    'mobilenetv2_dm05_coco_voc_trainval':\n","        'deeplabv3_mnv2_dm05_pascal_trainval',\n","    'mobilenetv2_coco_voctrainaug':\n","        'deeplabv3_mnv2_pascal_train_aug',\n","    'mobilenetv2_coco_voctrainval':\n","        'deeplabv3_mnv2_pascal_trainval',\n","    'xception_coco_voctrainaug':\n","        'deeplabv3_pascal_train_aug',\n","    'xception_coco_voctrainval':\n","        'deeplabv3_pascal_trainval',\n","}\n","_TARBALL_NAME = 'deeplab_model.tar.gz'\n","\n","MODEL_DIR = 'models'\n","tf.gfile.MakeDirs(MODEL_DIR)\n","\n","download_path = os.path.join(MODEL_DIR, _TARBALL_NAME)\n","print('downloading model, this might take a while...')\n","\n","urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n","                   download_path)\n","print('download completed.')\n","\n","with tarfile.open(download_path) as tar:\n","    tar.extractall(MODEL_DIR)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uudJWXwSvbDs","colab_type":"code","colab":{}},"source":["MODEL_PATH = os.path.join(MODEL_DIR, _MODEL_DIRS[MODEL_NAME])\n","FROZEN_GRAPH_NAME = 'frozen_inference_graph.pb'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBHs1iBH2_Aa","colab_type":"code","colab":{}},"source":["MODEL_NAME"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-cXrHRAy31V","colab_type":"code","colab":{}},"source":["os.path.join(MODEL_PATH, FROZEN_GRAPH_NAME)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGI92ddxybPG","colab_type":"code","colab":{}},"source":["deeplab_model = DeepLabModel(os.path.join(MODEL_PATH, FROZEN_GRAPH_NAME))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pn2QPI_vziiC","colab_type":"code","colab":{}},"source":["original_im = Image.open('parrot.jpg')\n","resized_im, seg_map = deeplab_model.run(original_im)\n","\n","vis_segmentation(resized_im, seg_map)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nEuFs8Jp0OKL","colab_type":"code","colab":{}},"source":["deeplab_model.run_benchmark(original_im)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2u0XmJCGPHi8","colab_type":"text"},"source":["# Load deeplab model and convert graph "]},{"cell_type":"code","metadata":{"id":"EGBnfeF9PHi9","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","graph = tf.Graph()\n","\n","graph_def = None\n","with tf.gfile.GFile(os.path.join(MODEL_PATH, FROZEN_GRAPH_NAME), 'rb') as f:\n","    graph_def = tf.GraphDef.FromString(f.read())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCNGrYlqPHjC","colab_type":"code","colab":{}},"source":["output_names = ['SemanticPredictions']\n","\n","converter = trt_convert.TrtGraphConverter(\n","    input_graph_def=graph_def,\n","    nodes_blacklist=output_names, #output nodes\n","    max_batch_size=1,\n","    is_dynamic_op=True,\n","    max_workspace_size_bytes=trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\n","    precision_mode=trt_convert.TrtPrecisionMode.FP16,\n","    minimum_segment_size=3)\n","\n","trt_graph = converter.convert()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AcD2ARZGUhkc","colab_type":"code","colab":{}},"source":["trt_engine_opts = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n","print(\"trt_engine_opts = {}\".format(trt_engine_opts))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYCAfQ15PHjA","colab_type":"code","colab":{}},"source":["with tf.gfile.GFile(os.path.join(MODEL_DIR, MODEL_NAME + '_fp16.pb'), 'wb') as f:\n","    f.write(trt_graph.SerializeToString())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0YpBb3PPHjH","colab_type":"text"},"source":["# Load FP16 Model and Run inference."]},{"cell_type":"code","metadata":{"id":"Bzsg-QG4qhwo","colab_type":"code","colab":{}},"source":["deeplab_fp16_model = DeepLabModel(os.path.join(MODEL_DIR, MODEL_NAME + '_fp16.pb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ux75P6ak2EC9","colab_type":"code","colab":{}},"source":["original_im = Image.open('parrot.jpg')\n","resized_im, seg_map = deeplab_model.run(original_im)\n","\n","vis_segmentation(resized_im, seg_map)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5wxYiT12H21","colab_type":"code","colab":{}},"source":["deeplab_model.run_benchmark(original_im)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4FnA_9TPHjH","colab_type":"code","colab":{}},"source":["INPUT_TENSOR_NAME = 'ImageTensor:0'\n","OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n","INPUT_SIZE = 513\n","FROZEN_GRAPH_NAME = 'frozen_inference_graph'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y9oxL1H5PHjK","colab_type":"code","colab":{}},"source":["def create_pascal_label_colormap():\n","    color_map = np.zeros((256, 3), dtype=int)\n","    ind = np.arange(256, dtype=int)\n","\n","    for shift in reversed(range(8)):\n","        for channel in range(3):\n","              color_map[:, channel] |= ((ind >> channel) & 1) << shift\n","        ind >>= 3\n","\n","    return color_map"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvtYlh8xPHjN","colab_type":"code","colab":{}},"source":["def label_to_color_image(label):\n","    if label.ndim != 2:\n","        raise ValueError('Expect 2-D input label')\n","\n","    color_map = create_pascal_label_colormap()\n","\n","    if np.max(label) >= len(color_map):\n","        raise ValueError('label value too large.')\n","\n","    return color_map[label]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gun3_WnbPHjQ","colab_type":"code","colab":{}},"source":["def vis_segmentation(image, seg_map):\n","    plt.figure(figsize=(15, 5))\n","    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n","\n","    plt.subplot(grid_spec[0])\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.title('input image')\n","\n","    plt.subplot(grid_spec[1])\n","    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n","    plt.imshow(seg_image)\n","    plt.axis('off')\n","    plt.title('segmentation map')\n","\n","    plt.subplot(grid_spec[2])\n","    plt.imshow(image)\n","    plt.imshow(seg_image, alpha=0.7)\n","    plt.axis('off')\n","    plt.title('segmentation overlay')\n","\n","    unique_labels = np.unique(seg_map)\n","    ax = plt.subplot(grid_spec[3])\n","    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n","    ax.yaxis.tick_right()\n","    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n","    plt.xticks([], [])\n","    ax.tick_params(width=0.0)\n","    plt.grid('off')\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mG_6odDePHjS","colab_type":"code","colab":{}},"source":["LABEL_NAMES = np.asarray([\n","    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n","    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n","])\n","\n","FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n","FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INdm0qPEPHjV","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fba8JTDSPHjX","colab_type":"code","colab":{}},"source":["graph = tf.Graph()\n","graph_def = None\n","\n","# with tf.gfile.GFile(os.path.join('./models', 'deeplabv3_mnv2_dm05_pascal_trainaug.pb'), 'rb') as f:\n","with tf.gfile.GFile(os.path.join('./models', 'deeplabv3_mnv2_dm05_pascal_trainaug_fp16.pb'), 'rb') as f:\n","    graph_def = tf.GraphDef.FromString(f.read())\n","    \n","if graph_def is None:\n","    print('Cannot read model.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVkCbEytPHjZ","colab_type":"code","colab":{}},"source":["with graph.as_default():\n","    tf.import_graph_def(graph_def, name='')\n","\n","    sess = tf.Session(graph=graph)\n","    \n","    # Read image.\n","    image = Image.open(os.path.join('./images', 'parrot.jpg'))\n","        \n","    width, height = image.size\n","    resize_ratio = 1.0 * INPUT_SIZE / max(width, height)\n","    target_size = (int(resize_ratio * width), int(resize_ratio * height))\n","    resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n","    \n","    batch_seg_map = sess.run(\n","        OUTPUT_TENSOR_NAME,\n","        feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n","    seg_map = batch_seg_map[0]\n","    \n","    print(seg_map.shape)\n","    \n","    vis_segmentation(resized_image, seg_map)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ns0pm0pkPHjb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AI99pOXgPHjd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}